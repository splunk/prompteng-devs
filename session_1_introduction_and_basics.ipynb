{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**You have an AI model. It seems to work. But how do you actually know?‚Äã**\n",
    "\n",
    "### Common Pain Points:\n",
    "- **Retrieval fails silently**: Gets irrelevant chunks but you don't notice\n",
    "- **Context gets lost**: Important info split across chunks disappears  \n",
    "- **Hallucination persists**: LLM makes up facts even with good sources\n",
    "- **Quality varies wildly**: Same question, different quality answers each time\n",
    "- **Manual checking doesn't scale**: Can't manually verify thousands of responses\n",
    "\n",
    "### The $10M Question:\n",
    "*\"How do you evaluate AI systems that generate nuanced, contextual responses at scale?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Evaluations Are Critical (Real-World Impact)\n",
    "\n",
    "print(\"üö® HIGH-STAKES AI DEPLOYMENT REALITY\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "deployment_stats = {\n",
    "    \"Customer Service Bots\": \"Handle millions of conversations daily\",\n",
    "    \"Content Moderation\": \"Process billions of social media posts\", \n",
    "    \"Medical AI\": \"Assist in patient diagnosis and treatment\",\n",
    "    \"Legal AI\": \"Evaluate document relevance in court cases\",\n",
    "    \"Financial AI\": \"Determine loan approvals and credit decisions\",\n",
    "    \"Educational AI\": \"Grade student work and provide feedback\"\n",
    "}\n",
    "\n",
    "print(\"Current AI Scale:\")\n",
    "for system, impact in deployment_stats.items():\n",
    "    print(f\"‚Ä¢ {system}: {impact}\")\n",
    "\n",
    "print(\"\\nüí∞ COST OF POOR EVALUATION:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "failure_costs = {\n",
    "    \"Customer Churn\": \"23% abandon AI tools after bad experience\",\n",
    "    \"Support Costs\": \"Poor AI increases human tickets by 40%\", \n",
    "    \"Brand Damage\": \"AI failures become viral social content\",\n",
    "    \"Legal Liability\": \"Biased systems face discrimination lawsuits\",\n",
    "    \"Regulatory Risk\": \"Can't prove compliance without measurement\"\n",
    "}\n",
    "\n",
    "for cost_type, impact in failure_costs.items():\n",
    "    print(f\"‚Ä¢ {cost_type}: {impact}\")\n",
    "\n",
    "print(\"\\nüéØ THE BOTTOM LINE:\")\n",
    "print(\"Without proper evaluation, AI systems fail silently at scale.\")\n",
    "print(\"LLM judges provide the solution - but only if built correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Traditional Evaluation Methods\n",
    "\n",
    "### Human Evaluation Methods:\n",
    "- **Expert assessment**: Manual rating but $5-50 per evaluation\n",
    "- **Weeks to scale**: Gold standard quality, impossible timeline\n",
    "- **Subjective bias**: Different evaluators, different standards\n",
    "- **Can't handle volume**: Thousands of outputs daily\n",
    "\n",
    "### Reference-Based Automated Metrics:\n",
    "- **Exact Match**: Perfect matches only, zero tolerance\n",
    "- **F1 Score**: Token overlap, misses meaning\n",
    "- **BLEU**: Translation metric, ignores factual accuracy\n",
    "- **ROUGE**: Content recall, can't detect hallucinations\n",
    "\n",
    "### Critical Limitations:\n",
    "- **Rigid scoring**: Correct rephrases score poorly\n",
    "- **Missing hallucination detection**: Can't spot made-up facts\n",
    "- **Context blind**: Ignores document grounding\n",
    "- **Too slow**: Can't monitor production systems real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Match (EM)\n",
    "\n",
    "Definition: Exact Match is a binary metric that determines if a generated text is perfectly identical to a reference text. It is a very strict measure, returning 1 (true) only if every character matches, including case, punctuation, and spacing; otherwise, it returns 0 (false). It has \"zero tolerance\" for any deviation.\n",
    "\n",
    "\n",
    "Formula:\n",
    "$$ EM(R, C) = \\begin{cases} 1 & \\text{if } R = C \\ 0 & \\text{if } R \\neq C \\end{cases} $$\n",
    "Where:\n",
    "\n",
    "\n",
    "$R$ is the Reference text.\n",
    "$C$ is the Candidate (generated) text.\n",
    "\n",
    "Exact Match is straightforward to implement manually or can be found in some NLP toolkits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: 'The capital of France is Paris.'\n",
      "Candidate 1: 'The capital of France is Paris.' -> EM Score: 1\n",
      "Candidate 2: 'The capital of France is paris.' -> EM Score: 0\n",
      "Candidate 3: 'Paris is the capital of France.' -> EM Score: 0\n"
     ]
    }
   ],
   "source": [
    "def exact_match(reference: str, candidate: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the Exact Match score between a reference and a candidate string.\n",
    "    Returns 1 if they are identical, 0 otherwise.\n",
    "    \"\"\"\n",
    "    return 1 if reference == candidate else 0\n",
    "\n",
    "# Working Example\n",
    "reference_em = \"The capital of France is Paris.\"\n",
    "\n",
    "candidate_em_1 = \"The capital of France is Paris.\"\n",
    "candidate_em_2 = \"The capital of France is paris.\"\n",
    "candidate_em_3 = \"Paris is the capital of France.\"\n",
    "\n",
    "print(f\"Reference: '{reference_em}'\")\n",
    "print(f\"Candidate 1: '{candidate_em_1}' -> EM Score: {exact_match(reference_em, candidate_em_1)}\")\n",
    "print(f\"Candidate 2: '{candidate_em_2}' -> EM Score: {exact_match(reference_em, candidate_em_2)}\")\n",
    "print(f\"Candidate 3: '{candidate_em_3}' -> EM Score: {exact_match(reference_em, candidate_em_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "Definition: The F1 Score is the harmonic mean of Precision and Recall. In the context of NLP text generation evaluation (especially for tasks like question answering where token overlap is important), it measures the overlap between the references in the generated text and the reference text.\n",
    "\n",
    "\n",
    "Precision: Measures how many of the references in the generated text are also present in the reference text. It answers: \"Of all the references I generated, how many were correct?\"\n",
    "Recall: Measures how many of the references in the reference text were captured by the generated text. It answers: \"Of all the correct references, how many did I generate?\"\n",
    "\n",
    "Formulas:\n",
    "Let:\n",
    "\n",
    "\n",
    "$TP$ (True Positives) = Number of references common to both the candidate and reference texts.\n",
    "$FP$ (False Positives) = Number of references in the candidate text but not in the reference text.\n",
    "$FN$ (False Negatives) = Number of references in the reference text but not in the candidate text.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} = \\frac{\\text{Number of matching references}}{\\text{Total references in candidate}} $$\n",
    "$$ Recall = \\frac{TP}{TP + FN} = \\frac{\\text{Number of matching references}}{\\text{Total references in reference}} $$\n",
    "$$ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $$\n",
    "\n",
    "For token-level F1, we often use sklearn.metrics.f1_score after converting strings to sets of references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reference references: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
      "Candidate references: ['a', 'quick', 'fox', 'jumps', 'over', 'a', 'dog.']\n",
      "F1 Score (token-level): 0.625\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def calculate_f1_score_references(reference_references: list, candidate_references: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the token-level F1 score between a reference and a candidate list of references.\n",
    "    \"\"\"\n",
    "    common = Counter(reference_references) & Counter(candidate_references)\n",
    "    num_common = sum(common.values())\n",
    "\n",
    "    if num_common == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision = num_common / len(candidate_references)\n",
    "    recall = num_common / len(reference_references)\n",
    "\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Working Example\n",
    "reference_f1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "candidate_f1 = \"A quick fox jumps over a dog.\"\n",
    "\n",
    "# Tokenize the sentences (simple split for demonstration)\n",
    "reference_references_f1 = reference_f1.lower().split()\n",
    "candidate_references_f1 = candidate_f1.lower().split()\n",
    "\n",
    "print(f\"\\nReference references: {reference_references_f1}\")\n",
    "print(f\"Candidate references: {candidate_references_f1}\")\n",
    "print(f\"F1 Score (token-level): {calculate_f1_score_references(reference_references_f1, candidate_references_f1):.3f}\")\n",
    "\n",
    "# Using sklearn for comparison (requires converting to binary labels, which is less direct for this specific use case)\n",
    "# For direct token overlap, the custom function above is more illustrative.\n",
    "# If using sklearn, it's typically for classification where each token is a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LLM as a Judge?\n",
    "\n",
    "Large Language Models (LLMs) as judges represent a paradigm where we leverage the reasoning capabilities of LLMs to evaluate, score, and assess various types of content, conversations, or decisions.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Automated Evaluation**: Replace human evaluators in specific contexts\n",
    "- **Consistent Scoring**: Provide standardized assessment criteria\n",
    "- **Scalable Assessment**: Handle large volumes of evaluation tasks\n",
    "- **Multi-dimensional Analysis**: Evaluate multiple criteria simultaneously\n",
    "\n",
    "### Why LLM Judges Changed Everything:\n",
    "- **Semantic Understanding**: Recognizes paraphrasing and meaning beyond keywords\n",
    "- **Scalable Human-like Judgment**: Thousands of evaluations in minutes vs weeks\n",
    "- **Reference-free Evaluation**: Can assess faithfulness without ground truth\n",
    "- **Contextual Assessment**: Considers domain expertise and user intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChatOllama initialized with llama3.1:8b model\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Optional\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize LLM\n",
    "try:\n",
    "    llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "    llm.invoke(\"Hello World!\")\n",
    "    print(\"‚úÖ ChatOllama initialized with llama3.1:8b model\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize ChatOllama: {e}\")\n",
    "    print(\"Please make sure Ollama is installed and running with llama3.1 model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: \n",
      "The quick brown fox jumps over the lazy dog. This sentence contains all letters of the alphabet.\n",
      "It's commonly used for testing fonts and keyboards.\n",
      "\n",
      "\n",
      "Evaluation Criteria:\n",
      "- Clarity: How clear and understandable is the text?\n",
      "- Informativeness: How much useful information does it provide?\n",
      "- Engagement: How engaging is the content for readers?\n"
     ]
    }
   ],
   "source": [
    "# Simple example of LLM evaluation concept\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This sentence contains all letters of the alphabet.\n",
    "It's commonly used for testing fonts and keyboards.\n",
    "\"\"\"\n",
    "\n",
    "evaluation_criteria = {\n",
    "    \"clarity\": \"How clear and understandable is the text?\",\n",
    "    \"informativeness\": \"How much useful information does it provide?\",\n",
    "    \"engagement\": \"How engaging is the content for readers?\"\n",
    "}\n",
    "\n",
    "print(\"Sample Text:\", sample_text)\n",
    "print(\"\\nEvaluation Criteria:\")\n",
    "for criterion, description in evaluation_criteria.items():\n",
    "    print(f\"- {criterion.title()}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM EVALUATION RESULTS\n",
      "\n",
      "üéØ Evaluating: Clarity\n",
      "----------------------------------------\n",
      "LLM Response:\n",
      "Score: 9/10\n",
      "Reasoning: The text is clear and easy to understand, but it assumes some prior knowledge about the purpose of the sentence. A reader who has never heard of this sentence before might not fully grasp its significance or why it's used for testing fonts and keyboards. However, the language itself is simple and straightforward, making it accessible to a wide range of readers.\n",
      "\n",
      "üéØ Evaluating: Informativeness\n",
      "----------------------------------------\n",
      "LLM Response:\n",
      "Score: 6/10\n",
      "Reasoning: The text provides some useful information about the sentence, specifically its use for testing fonts and keyboards. However, it doesn't provide much depth or context beyond that. It also assumes prior knowledge of why this particular sentence is significant (i.e., containing all letters of the alphabet), which limits its usefulness to readers who are already familiar with this fact.\n",
      "\n",
      "üéØ Evaluating: Engagement\n",
      "----------------------------------------\n",
      "LLM Response:\n",
      "Score: 2/10\n",
      "Reasoning: The content is dry and lacks any narrative or emotional appeal. It's primarily informative, stating a fact about the sentence's composition and its practical application. While it may be interesting for those who appreciate linguistic trivia, it's unlikely to engage readers on an emotional level or spark their curiosity in a significant way.\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ LLM EVALUATION RESULTS\")\n",
    "# Now let's use the LLM to evaluate the text against each criterion\n",
    "for criterion, description in evaluation_criteria.items():\n",
    "    print(f\"\\nüéØ Evaluating: {criterion.title()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create evaluation prompt\n",
    "    evaluation_prompt = f\"\"\"\n",
    "Please evaluate the following text based on this criterion: {description}\n",
    "\n",
    "Text to evaluate: {sample_text.strip()}\n",
    "\n",
    "Provide a score from 1-10 and a brief explanation of your reasoning.\n",
    "Format your response as:\n",
    "Score: X/10\n",
    "Reasoning: [Your explanation]\n",
    "\"\"\"\n",
    "    \n",
    "    # Get LLM evaluation\n",
    "    try:\n",
    "        response = llm.invoke(evaluation_prompt)\n",
    "        print(f\"LLM Response:\\n{response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications Across Domains\n",
    "\n",
    "### Legal and Judicial Applications\n",
    "- **Document Relevance Scoring**: Assess relevance of legal documents to cases\n",
    "- **Case Law Analysis**: Evaluate similarity between legal precedents\n",
    "- **Judicial Decision Support**: Assist in evidence evaluation and consistency checking\n",
    "\n",
    "### Content Quality Evaluation\n",
    "- **Academic Paper Review**: Automated initial screening of research papers\n",
    "- **Content Moderation**: Scale content review for platforms\n",
    "- **Customer Service Quality**: Evaluate support interactions\n",
    "\n",
    "### Conversation Assessment\n",
    "- **Chatbot Performance**: Evaluate AI assistant responses\n",
    "- **Human-likeness Detection**: Assess naturalness of generated conversations\n",
    "- **Training Data Quality**: Validate synthetic conversation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèõÔ∏è LEGAL DOMAIN EXAMPLE: Document Relevance Scoring\n",
      "============================================================\n",
      "Case: Personal injury lawsuit: slip and fall at grocery store\n",
      "Document: Store surveillance footage showing wet floor conditions on day of incident\n",
      "\n",
      "ü§ñ LLM Evaluation:\n",
      "I would rate the document's relevance to the legal case as a 9 out of 10.\n",
      "\n",
      "The document is directly related to the incident in question, providing visual evidence of the store's condition at the time of the slip and fall. The footage can be used to:\n",
      "\n",
      "* Support or refute claims made by the plaintiff about the cause of the accident\n",
      "* Show that the store was aware of the wet floor conditions and failed to take adequate measures to address them\n",
      "* Demonstrate the extent of the hazard posed by the wet floor\n",
      "\n",
      "The only reason I wouldn't give it a perfect 10 is that, without more context or analysis, we can't be certain what specific details the footage shows. However, in general, store surveillance footage is highly relevant and probative evidence in slip and fall cases like this one.\n",
      "\n",
      "üí¨ CONVERSATION EXAMPLE: Chatbot Response Quality\n",
      "============================================================\n",
      "Customer: I ordered a laptop 3 days ago but haven't received shipping confirmation. Can you help?\n",
      "Chatbot: Orders usually ship within 5-7 business days. Please wait longer.\n",
      "\n",
      "ü§ñ LLM Evaluation:\n",
      "I would rate the helpfulness of this chatbot response as a 2 out of 10.\n",
      "\n",
      "The response is unhelpful for several reasons:\n",
      "\n",
      "* It doesn't acknowledge the customer's concern or frustration about not receiving shipping confirmation.\n",
      "* The answer is too vague, stating only that orders \"usually\" ship within 5-7 business days. This doesn't provide any specific information about the status of this particular order.\n",
      "* The response essentially tells the customer to wait longer without offering any additional assistance or next steps.\n",
      "\n",
      "To improve this response, I would suggest the following:\n",
      "\n",
      "1. Acknowledge the customer's concern: \"Sorry to hear that you haven't received shipping confirmation yet.\"\n",
      "2. Provide a more specific answer: \"I've checked on your order and it was shipped out yesterday. You should receive an email with tracking information shortly.\"\n",
      "3. Offer additional assistance or next steps: \"If you don't receive the email within the next 24 hours, please let me know and I'll be happy to look into this further.\"\n",
      "\n",
      "Here's an example of a rewritten response that addresses these issues:\n",
      "\n",
      "\"Sorry to hear that you haven't received shipping confirmation yet. I've checked on your order and it was shipped out yesterday. You should receive an email with tracking information shortly. If you don't receive the email within the next 24 hours, please let me know and I'll be happy to look into this further.\"\n",
      "\n",
      "üí° Key Takeaways:\n",
      "- Legal: Helps prioritize case materials\n",
      "- Chatbot: Improves customer service quality\n",
      "- All domains need clear evaluation criteria!\n"
     ]
    }
   ],
   "source": [
    "# Domain Examples for LLM as Judge\n",
    "\n",
    "print(\"üèõÔ∏è LEGAL DOMAIN EXAMPLE: Document Relevance Scoring\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Case scenario\n",
    "legal_case = \"Personal injury lawsuit: slip and fall at grocery store\"\n",
    "sample_document = \"Store surveillance footage showing wet floor conditions on day of incident\"\n",
    "\n",
    "print(f\"Case: {legal_case}\")\n",
    "print(f\"Document: {sample_document}\")\n",
    "\n",
    "# LLM evaluation\n",
    "legal_prompt = f\"\"\"\n",
    "Rate this document's relevance to the legal case (1-10 scale):\n",
    "\n",
    "Case: {legal_case}\n",
    "Document: {sample_document}\n",
    "\n",
    "Provide: Score (1-10) and brief reasoning.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    legal_result = llm.invoke(legal_prompt)\n",
    "    print(f\"\\nü§ñ LLM Evaluation:\\n{legal_result.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nüí¨ CONVERSATION EXAMPLE: Chatbot Response Quality\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Customer service scenario\n",
    "customer_query = \"I ordered a laptop 3 days ago but haven't received shipping confirmation. Can you help?\"\n",
    "chatbot_response = \"Orders usually ship within 5-7 business days. Please wait longer.\"\n",
    "\n",
    "print(f\"Customer: {customer_query}\")\n",
    "print(f\"Chatbot: {chatbot_response}\")\n",
    "\n",
    "# LLM evaluation\n",
    "chatbot_prompt = f\"\"\"\n",
    "Evaluate this chatbot response for customer service quality:\n",
    "\n",
    "Customer Query: {customer_query}\n",
    "Chatbot Response: {chatbot_response}\n",
    "\n",
    "Rate helpfulness (1-10) and suggest improvements.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    chatbot_result = llm.invoke(chatbot_prompt)\n",
    "    print(f\"\\nü§ñ LLM Evaluation:\\n{chatbot_result.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nüí° Key Takeaways:\")\n",
    "print(\"- Legal: Helps prioritize case materials\")\n",
    "print(\"- Chatbot: Improves customer service quality\")\n",
    "print(\"- All domains need clear evaluation criteria!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù CONTENT QUALITY EXAMPLE: Academic Paper Review\n",
      "============================================================\n",
      "Abstract to review:\n",
      "\n",
      "We surveyed 10 students about social media and mood. Students using social media \n",
      "more than 3 hours daily sometimes felt sad. Therefore, social media is bad for \n",
      "all teenagers and should be banned.\n",
      "\n",
      "\n",
      "ü§ñ LLM Review:\n",
      "I'd rate this abstract a 2 out of 10 in terms of quality.\n",
      "\n",
      "Here are the main problems I've identified:\n",
      "\n",
      "1. **Sample size**: The sample size is extremely small, consisting of only 10 students. This is not sufficient to draw any meaningful conclusions about social media use and mood among teenagers.\n",
      "2. **Lack of control group**: There is no comparison group or control condition in this study. How do we know that the students who used social media more than 3 hours a day would have felt sad if they hadn't used social media? A control group would help to establish causality.\n",
      "3. **Correlation vs. causation**: The abstract implies that using social media causes sadness, but it's possible that there are other factors at play (e.g., students who use social media more may be more prone to depression or anxiety). Correlational studies like this one can't establish cause-and-effect relationships.\n",
      "4. **Overly broad conclusion**: The abstract concludes that \"social media is bad for all teenagers and should be banned.\" This is an overly simplistic and sweeping statement, especially given the small sample size and lack of control group.\n",
      "5. **Lack of statistical analysis**: There's no mention of any statistical tests or analyses used to examine the relationship between social media use and mood. This makes it difficult to evaluate the validity of the findings.\n",
      "\n",
      "Overall, this abstract raises more questions than answers, and its conclusions are likely based on a flawed methodology.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìù CONTENT QUALITY EXAMPLE: Academic Paper Review\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample abstract with obvious flaws\n",
    "paper_abstract = \"\"\"\n",
    "We surveyed 10 students about social media and mood. Students using social media \n",
    "more than 3 hours daily sometimes felt sad. Therefore, social media is bad for \n",
    "all teenagers and should be banned.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Abstract to review:\\n{paper_abstract}\")\n",
    "\n",
    "# LLM evaluation\n",
    "academic_prompt = f\"\"\"\n",
    "Review this academic abstract for quality issues:\n",
    "\n",
    "Abstract: {paper_abstract}\n",
    "\n",
    "Rate (1-10) and identify main problems with methodology, sample size, or conclusions.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    academic_result = llm.invoke(academic_prompt)\n",
    "    print(f\"\\nü§ñ LLM Review:\\n{academic_result.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ CONVERSATION EXAMPLE: Chatbot Response Quality\n",
      "============================================================\n",
      "Customer: I ordered a laptop 3 days ago but haven't received shipping confirmation. Can you help?\n",
      "Chatbot: Orders usually ship within 5-7 business days. Please wait longer.\n",
      "\n",
      "ü§ñ LLM Evaluation:\n",
      "I would rate the helpfulness of this chatbot response as a 2 out of 10.\n",
      "\n",
      "The response is unhelpful for several reasons:\n",
      "\n",
      "* It doesn't acknowledge the customer's concern or frustration about not receiving shipping confirmation.\n",
      "* The answer is too vague, stating only that orders \"usually\" ship within 5-7 business days. This doesn't provide any specific information about the status of this particular order.\n",
      "* The response essentially tells the customer to wait longer without offering any additional assistance or next steps.\n",
      "\n",
      "To improve this response, I would suggest the following:\n",
      "\n",
      "1. Acknowledge the customer's concern: \"Sorry to hear that you haven't received shipping confirmation yet.\"\n",
      "2. Provide a more specific answer: \"I've checked on your order and it was shipped out yesterday. You should receive an email with tracking information shortly.\"\n",
      "3. Offer additional assistance or next steps: \"If you don't receive the email within the next 24 hours, please let me know and I'll be happy to look into this further.\"\n",
      "\n",
      "Here's an example of a rewritten response that addresses these issues:\n",
      "\n",
      "\"Sorry to hear that you haven't received shipping confirmation yet. I've checked on your order and it was shipped out yesterday. You should receive an email with tracking information shortly. If you don't receive the email within the next 24 hours, please let me know and I'll be happy to look into this further.\"\n",
      "\n",
      "============================================================\n",
      "üí° Key Takeaways:\n",
      "- Legal: Helps prioritize case materials\n",
      "- Academic: Catches obvious methodology flaws\n",
      "- Chatbot: Improves customer service quality\n",
      "- All domains need clear evaluation criteria!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüí¨ CONVERSATION EXAMPLE: Chatbot Response Quality\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Customer service scenario\n",
    "customer_query = \"I ordered a laptop 3 days ago but haven't received shipping confirmation. Can you help?\"\n",
    "chatbot_response = \"Orders usually ship within 5-7 business days. Please wait longer.\"\n",
    "\n",
    "print(f\"Customer: {customer_query}\")\n",
    "print(f\"Chatbot: {chatbot_response}\")\n",
    "\n",
    "# LLM evaluation\n",
    "chatbot_prompt = f\"\"\"\n",
    "Evaluate this chatbot response for customer service quality:\n",
    "\n",
    "Customer Query: {customer_query}\n",
    "Chatbot Response: {chatbot_response}\n",
    "\n",
    "Rate helpfulness (1-10) and suggest improvements.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    chatbot_result = llm.invoke(chatbot_prompt)\n",
    "    print(f\"\\nü§ñ LLM Evaluation:\\n{chatbot_result.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° Key Takeaways:\")\n",
    "print(\"- Legal: Helps prioritize case materials\")\n",
    "print(\"- Academic: Catches obvious methodology flaws\") \n",
    "print(\"- Chatbot: Improves customer service quality\")\n",
    "print(\"- All domains need clear evaluation criteria!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Journey Most People Take (And Why It's Problematic)\n",
    "\n",
    "Most people start with LLM judging the same way:\n",
    "1. **\"Just ask if it's correct\"** - Seems obvious, what could go wrong?\n",
    "2. **\"Ask for true/false\"** - More structured, feels better\n",
    "3. **\"Give it a score\"** - Numbers feel objective and scientific\n",
    "4. **\"Compare two options\"** - Let the LLM pick the better one\n",
    "\n",
    "**Spoiler**: Each approach has serious hidden flaws that most people never discover.\n",
    "\n",
    "Let's experience this journey together, starting with the most naive approach..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach 1: \"Just Tell Me If This Answer Is Correct\"\n",
    "\n",
    "This is how everyone starts. It seems so simple and obvious:\n",
    "- Give the LLM a question and an answer\n",
    "- Ask \"Is this answer correct?\"\n",
    "- Trust the yes/no response\n",
    "\n",
    "### What Could Possibly Go Wrong?\n",
    "Let's find out using carefully chosen examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Naive Approach 1: 'Just Tell Me If This Answer Is Correct'\n",
      "This method fails spectacularly when the LLM is presented with information that is both plausible and incorrect. The LLM may lack the internal process to critically verify a statement that it is presented with as fact, especially if the answer is short and lacks context.\n",
      "\n",
      "User Question: What programming language should beginners learn first?\n",
      "Model Answer: Python is an excellent choice for beginners because it has clean, readable syntax and a gentle learning curve. Most computer science courses and coding bootcamps start with Python.\n",
      "LLM Judge Prompt:\n",
      "\n",
      "Is the given answer correct? Only answer with Yes or No.\n",
      "Question: 'What programming language should beginners learn first?'\n",
      "Answer: 'Python is an excellent choice for beginners because it has clean, readable syntax and a gentle learning curve. Most computer science courses and coding bootcamps start with Python.'\n",
      "\n",
      "LLM Response: content='Yes' additional_kwargs={} response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-09-17T05:43:50.950278Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1307772584, 'load_duration': 886591917, 'prompt_eval_count': 71, 'prompt_eval_duration': 399741583, 'eval_count': 2, 'eval_duration': 20468667, 'model_name': 'llama3.1:8b'} id='run--4c59488c-c22e-4ea3-b3cb-059760c9bf78-0' usage_metadata={'input_tokens': 71, 'output_tokens': 2, 'total_tokens': 73}\n",
      "\n",
      "**What goes wrong:** The LLM will often agree even if the answer is subjective and other equally valid answers exist. It can confidently state 'yes' to an opinion presented as fact, making the output seem reliable when it is not universally true.\n",
      "**Hidden Flaw:** The LLM's confidence is not a reliable indicator of correctness when the question itself is subjective. Confident, research-backed language can trick the LLM into thinking advice is factual rather than contextual.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Naive Approach 1: 'Just Tell Me If This Answer Is Correct'\")\n",
    "print(\"This method fails spectacularly when the LLM is presented with information that is both plausible and incorrect. The LLM may lack the internal process to critically verify a statement that it is presented with as fact, especially if the answer is short and lacks context.\")\n",
    "\n",
    "# User's example details\n",
    "user_question_1 = \"What programming language should beginners learn first?\"\n",
    "model_answer_1 = \"Python is an excellent choice for beginners because it has clean, readable syntax and a gentle learning curve. Most computer science courses and coding bootcamps start with Python.\"\n",
    "llm_judge_prompt_1 = f\"\"\"\n",
    "Is the given answer correct? Only answer with Yes or No.\n",
    "Question: '{user_question_1}'\n",
    "Answer: '{model_answer_1}'\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nUser Question: {user_question_1}\")\n",
    "print(f\"Model Answer: {model_answer_1}\")\n",
    "print(f\"LLM Judge Prompt:\\n{llm_judge_prompt_1}\")\n",
    "\n",
    "# Simulating LLM response based on the problem description\n",
    "# In a real scenario, llm.invoke(llm_judge_prompt_2) would be called.\n",
    "# The problem description states: \"LLM likely says YES because answer sounds authoritative and mentions 'most courses' - mistaking common practice for universal truth.\"\n",
    "response_1_content = llm.invoke(llm_judge_prompt_1)\n",
    "response_1 = type('obj', (object,), {'content': response_1_content})() # Mocking the response object\n",
    "print(f\"LLM Response: {response_1.content}\")\n",
    "\n",
    "print(\"\\n**What goes wrong:** The LLM will often agree even if the answer is subjective and other equally valid answers exist. It can confidently state 'yes' to an opinion presented as fact, making the output seem reliable when it is not universally true.\")\n",
    "print(\"**Hidden Flaw:** The LLM's confidence is not a reliable indicator of correctness when the question itself is subjective. Confident, research-backed language can trick the LLM into thinking advice is factual rather than contextual.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach 2: True/False Classification\n",
    "\n",
    "After discovering issues with simple correctness, people often move to true/false evaluation:\n",
    "- Seems more structured and binary\n",
    "- Feels more \"scientific\" than yes/no\n",
    "- But loses important nuance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Naive Approach 3: 'True/False with Nuanced Claims'\n",
      "This method fails when the LLM is asked to evaluate a nuanced claim as a simple true or false statement, even with context. The LLM may lack the ability to acknowledge the complexities, exceptions, or varying degrees of truth within a statement, leading to an oversimplified 'True' or 'False' response that misses critical subtleties.\n",
      "\n",
      "User Question: Is the following statement true or false given the context?\n",
      "Statement being evaluated (Model Answer): Exercise is good for mental health\n",
      "Context provided: Regular moderate exercise has been shown in numerous studies to reduce symptoms of depression and anxiety.\n",
      "LLM Judge Prompt:\n",
      "\n",
      "Is the following statement true or false given the context? Return only True or False.\n",
      "Statement: 'Exercise is good for mental health'\n",
      "Context: 'Regular moderate exercise has been shown in numerous studies to reduce symptoms of depression and anxiety.'\n",
      "\n",
      "LLM Response: content='True.' additional_kwargs={} response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-09-17T05:44:03.94595Z', 'done': True, 'done_reason': 'stop', 'total_duration': 432792000, 'load_duration': 74965458, 'prompt_eval_count': 57, 'prompt_eval_duration': 314256000, 'eval_count': 3, 'eval_duration': 42852458, 'model_name': 'llama3.1:8b'} id='run--aaa08250-df9a-4fde-af32-16148e2dca89-0' usage_metadata={'input_tokens': 57, 'output_tokens': 3, 'total_tokens': 60}\n",
      "\n",
      "**What goes wrong:** The LLM will likely respond 'True' because the statement is broadly accepted, despite the significant nuances and exceptions. It oversimplifies a complex topic into a binary answer.\n",
      "**Hidden Flaw:** The LLM's binary 'True/False' judgment fails to capture the conditional nature or limitations of the claim. It struggles with statements that are 'mostly true' but not universally or unconditionally true, especially when the context provided supports the general truth without elaborating on exceptions.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Naive Approach 2: 'True/False with Nuanced Claims'\")\n",
    "print(\"This method fails when the LLM is asked to evaluate a nuanced claim as a simple true or false statement, even with context. The LLM may lack the ability to acknowledge the complexities, exceptions, or varying degrees of truth within a statement, leading to an oversimplified 'True' or 'False' response that misses critical subtleties.\")\n",
    "\n",
    "# User's example details\n",
    "user_question_2 = \"Is the following statement true or false given the context?\"\n",
    "model_answer_2 = \"Exercise is good for mental health\" # This is the statement being evaluated\n",
    "context_2 = \"Regular moderate exercise has been shown in numerous studies to reduce symptoms of depression and anxiety.\"\n",
    "llm_judge_prompt_2 = f\"\"\"\n",
    "Is the following statement true or false given the context? Return only True or False.\n",
    "Statement: '{model_answer_2}'\n",
    "Context: '{context_2}'\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nUser Question: {user_question_2}\")\n",
    "print(f\"Statement being evaluated (Model Answer): {model_answer_2}\")\n",
    "print(f\"Context provided: {context_2}\")\n",
    "print(f\"LLM Judge Prompt:\\n{llm_judge_prompt_2}\")\n",
    "\n",
    "# Simulating LLM response based on the problem description\n",
    "# The problem description states: \"Generally true, but ignores individual variation, severity of conditions, and that exercise alone isn't sufficient for serious mental health issues\"\n",
    "# An LLM would likely respond 'True' because the statement is generally accepted as true, overlooking the nuances.\n",
    "response_2_content = llm.invoke(llm_judge_prompt_2)\n",
    "response_2 = type('obj', (object,), {'content': response_2_content})() # Mocking the response object\n",
    "print(f\"LLM Response: {response_2.content}\")\n",
    "\n",
    "print(\"\\n**What goes wrong:** The LLM will likely respond 'True' because the statement is broadly accepted, despite the significant nuances and exceptions. It oversimplifies a complex topic into a binary answer.\")\n",
    "print(\"**Hidden Flaw:** The LLM's binary 'True/False' judgment fails to capture the conditional nature or limitations of the claim. It struggles with statements that are 'mostly true' but not universally or unconditionally true, especially when the context provided supports the general truth without elaborating on exceptions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach 3: Direct Scoring\n",
    "\n",
    "When true/false feels too limiting, people turn to scoring:\n",
    "- \"Numbers are objective!\"\n",
    "- \"1-10 scale feels scientific\"\n",
    "- But without clear criteria, scores become arbitrary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Naive Approach 3: Direct Scoring\n",
      "A score can be completely arbitrary without a clear definition of what each number represents. A partially correct answer or even a hallucinated answer might receive a surprisingly high score if the LLM is designed to be helpful rather than strictly accurate.\n",
      "\n",
      "User Question: Explain how photosynthesis works\n",
      "Model Answer: Plants use sunlight to make food. Chlorophyll in leaves absorbs light and converts carbon dioxide and water into glucose and oxygen.\n",
      "LLM Judge Prompt:\n",
      "\n",
      "Rate this answer from 1-10 for quality. Only provide the number. Explain your reasoning.\n",
      "Question: 'Explain how photosynthesis works'\n",
      "Answer: 'Plants use sunlight to make food. Chlorophyll in leaves absorbs light and converts carbon dioxide and water into glucose and oxygen.'\n",
      "\n",
      "LLM Response (Example Run 1): content='8\\n\\nThis answer provides a clear and concise explanation of the basic process of photosynthesis, including the key components involved (chlorophyll, sunlight, CO2, H2O, glucose, and O2). However, it lacks detail and does not mention the overall equation for photosynthesis or the role of light-dependent reactions.' additional_kwargs={} response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-09-17T05:45:08.176698Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1674546042, 'load_duration': 76024167, 'prompt_eval_count': 69, 'prompt_eval_duration': 308406792, 'eval_count': 67, 'eval_duration': 1289491833, 'model_name': 'llama3.1:8b'} id='run--f9fb30a3-81a1-465d-9535-53a242ab2cbf-0' usage_metadata={'input_tokens': 69, 'output_tokens': 67, 'total_tokens': 136}\n",
      "\n",
      "**Hidden Flaw:** The LLM lacks a transparent and consistently applied internal rubric for 'quality'. Without explicit criteria provided in the prompt, its scoring becomes arbitrary, reflecting internal stochasticity rather than a stable evaluation of the answer's merit. This means 'no clear criteria means arbitrary scoring'.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Naive Approach 3: Direct Scoring\")\n",
    "print(\"A score can be completely arbitrary without a clear definition of what each number represents. A partially correct answer or even a hallucinated answer might receive a surprisingly high score if the LLM is designed to be helpful rather than strictly accurate.\")\n",
    "# User's example details\n",
    "user_question_3 = \"Explain how photosynthesis works\"\n",
    "model_answer_3 = \"Plants use sunlight to make food. Chlorophyll in leaves absorbs light and converts carbon dioxide and water into glucose and oxygen.\"\n",
    "llm_judge_prompt_3 = f\"\"\"\n",
    "Rate this answer from 1-10 for quality. Only provide the number. Explain your reasoning.\n",
    "Question: '{user_question_3}'\n",
    "Answer: '{model_answer_3}'\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nUser Question: {user_question_3}\")\n",
    "print(f\"Model Answer: {model_answer_3}\")\n",
    "print(f\"LLM Judge Prompt:\\n{llm_judge_prompt_3}\")\n",
    "\n",
    "# Simulating LLM response based on the problem description\n",
    "# The problem description explicitly shows inconsistency by running multiple times.\n",
    "# We'll simulate one run and then explain the inconsistency in the analysis.\n",
    "# Let's pick a plausible score for a single run.\n",
    "response_3= llm.invoke(llm_judge_prompt_3)\n",
    "response_3 = type('obj', (object,), {'content': response_3 })() # Mocking the response object\n",
    "print(f\"LLM Response (Example Run 1): {response_3.content}\")\n",
    "\n",
    "# To demonstrate inconsistency as per the original example, we would run it multiple times:\n",
    "# For illustrative purposes in the explanation, we can mention a range.\n",
    "# Example scores from multiple runs could be 8, 7, 9.\n",
    "# print(f\"LLM Response (Example Run 2): 7\")\n",
    "# print(f\"LLM Response (Example Run 3): 9\")\n",
    "\n",
    "print(\"\\n**Hidden Flaw:** The LLM lacks a transparent and consistently applied internal rubric for 'quality'. Without explicit criteria provided in the prompt, its scoring becomes arbitrary, reflecting internal stochasticity rather than a stable evaluation of the answer's merit. This means 'no clear criteria means arbitrary scoring'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach 4: Compare two options\n",
    "\n",
    "When direct scoring proves too arbitrary and inconsistent, people often pivot to comparing options:\n",
    "\n",
    "\n",
    "\"Let the LLM pick the better one!\"\n",
    "\"It's how humans often evaluate choices, so it must be good.\"\n",
    "But this method is still susceptible to subtle biases that can skew the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Naive Approach 4: Compare two options\n",
      "This method, popular in preference fine-tuning, is still susceptible to several biases, including position bias and verbosity bias. An LLM might prefer a response based on its length or its position in the prompt rather than its actual quality.\n",
      "\n",
      "User Question: Describe the city of New York.\n",
      "Model Answer A: New York City is a global hub for finance, culture, and media. It is the most populous city in the United States and is home to iconic landmarks like the Statue of Liberty and Times Square. Its diverse neighborhoods and bustling atmosphere make it a unique and dynamic place to visit.\n",
      "Model Answer B: New York City is a place with lots of tall buildings and famous spots. It has many different people from all over the world. It can be a very busy and exciting place to be.\n",
      "LLM Judge Prompt (A first):\n",
      "Which of the two following answers is better?\n",
      "Answer A: 'New York City is a global hub for finance, culture, and media. It is the most populous city in the United States and is home to iconic landmarks like the Statue of Liberty and Times Square. Its diverse neighborhoods and bustling atmosphere make it a unique and dynamic place to visit.'\n",
      "Answer B: 'New York City is a place with lots of tall buildings and famous spots. It has many different people from all over the world. It can be a very busy and exciting place to be.'\n",
      "LLM Response (A first): After analyzing both answers, I would say that **Answer A** is better for several reasons:\n",
      "\n",
      "1. **Specificity**: Answer A provides specific examples of New York City's significance (finance, culture, media) and iconic landmarks (Statue of Liberty, Times Square), making it more informative and engaging.\n",
      "2. **Vivid language**: The use of words like \"global hub,\" \"iconic,\" \"diverse neighborhoods,\" and \"bustling atmosphere\" creates a richer and more immersive experience for the reader.\n",
      "3. **Clear structure**: Answer A follows a logical structure, starting with an overview of New York City's significance and then highlighting its notable features.\n",
      "4. **Engagement**: The description in Answer A is likely to pique the interest of readers who are interested in travel or learning about new places.\n",
      "\n",
      "In contrast, Answer B:\n",
      "\n",
      "1. **Lacks specificity**: It uses vague terms like \"lots of tall buildings\" and \"famous spots,\" which don't provide much insight into what makes New York City unique.\n",
      "2. **Uses simple language**: While simplicity can be beneficial for some audiences, it doesn't add depth or interest to the description in this case.\n",
      "3. **Fails to engage**: The description is more generic and less likely to capture the reader's attention.\n",
      "\n",
      "Overall, Answer A provides a more detailed, engaging, and informative description of New York City, making it the better choice.\n",
      "\n",
      "**Hidden flaw (Verbosity Bias, as per your description):** The judge will almost certainly favor the longer, more verbose Answer A, associating greater length with higher quality, even though Answer B is not necessarily wrong or inadequate for certain contexts. The LLM's response often reflects this preference by citing more detail or sophisticated language.\n",
      "\n",
      "LLM Judge Prompt (B first):\n",
      "Which of the two following answers is better?\n",
      "Answer A: 'New York City is a place with lots of tall buildings and famous spots. It has many different people from all over the world. It can be a very busy and exciting place to be.'\n",
      "Answer B: 'New York City is a global hub for finance, culture, and media. It is the most populous city in the United States and is home to iconic landmarks like the Statue of Liberty and Times Square. Its diverse neighborhoods and bustling atmosphere make it a unique and dynamic place to visit.'\n",
      "LLM Response (B first): Answer B is significantly better than Answer A for several reasons:\n",
      "\n",
      "1. **Specificity**: Answer B provides specific details about New York City, such as its status as a global hub for finance, culture, and media, which gives the reader a clearer understanding of what the city has to offer.\n",
      "2. **Accuracy**: The information in Answer B is more accurate and up-to-date compared to Answer A. For example, it correctly identifies New York City as the most populous city in the United States (although this may change over time).\n",
      "3. **Organization**: Answer B presents its information in a clear and organized manner, making it easier for the reader to follow.\n",
      "4. **Style**: The language used in Answer B is more formal and polished than Answer A, which makes it more suitable for academic or professional writing.\n",
      "\n",
      "Answer A, on the other hand, is more general and lacks specific details about New York City. It also uses vague terms like \"lots of tall buildings\" and \"famous spots,\" which don't provide much insight into what makes the city unique.\n",
      "\n",
      "Overall, Answer B is a better choice because it provides more accurate, specific, and organized information that gives the reader a clearer understanding of New York City's characteristics.\n",
      "\n",
      "**Hidden flaw (Position Bias, as per your description):** If the order of the answers were swapped (as demonstrated in the second prompt), there is a chance the LLM would favor the new 'Answer A' (which is now the less verbose one), demonstrating an unconscious preference for the first item presented. This bias is particularly problematic for instances where the answers are of similar quality, as the LLM's preference can be swayed by the arbitrary ordering.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Naive Approach 4: Compare two options\")\n",
    "print(\"This method, popular in preference fine-tuning, is still susceptible to several biases, including position bias and verbosity bias. An LLM might prefer a response based on its length or its position in the prompt rather than its actual quality.\")\n",
    "\n",
    "# User's example details\n",
    "user_question_4 = \"Describe the city of New York.\"\n",
    "model_answer_A_4 = \"New York City is a global hub for finance, culture, and media. It is the most populous city in the United States and is home to iconic landmarks like the Statue of Liberty and Times Square. Its diverse neighborhoods and bustling atmosphere make it a unique and dynamic place to visit.\"\n",
    "model_answer_B_4 = \"New York City is a place with lots of tall buildings and famous spots. It has many different people from all over the world. It can be a very busy and exciting place to be.\"\n",
    "\n",
    "# Prompt for Verbosity Bias (A is more verbose)\n",
    "llm_judge_prompt_4_verbosity = f\"\"\"Which of the two following answers is better?\n",
    "Answer A: '{model_answer_A_4}'\n",
    "Answer B: '{model_answer_B_4}'\"\"\"\n",
    "\n",
    "print(f\"\\nUser Question: {user_question_4}\")\n",
    "print(f\"Model Answer A: {model_answer_A_4}\")\n",
    "print(f\"Model Answer B: {model_answer_B_4}\")\n",
    "print(f\"LLM Judge Prompt (A first):\\n{llm_judge_prompt_4_verbosity}\")\n",
    "\n",
    "response_4_verbosity = llm.invoke(llm_judge_prompt_4_verbosity)\n",
    "print(f\"LLM Response (A first): {response_4_verbosity.content}\")\n",
    "\n",
    "print(\"\\n**Hidden flaw (Verbosity Bias, as per your description):** The judge will almost certainly favor the longer, more verbose Answer A, associating greater length with higher quality, even though Answer B is not necessarily wrong or inadequate for certain contexts. The LLM's response often reflects this preference by citing more detail or sophisticated language.\")\n",
    "\n",
    "# Prompt for Position Bias (swapping A and B)\n",
    "llm_judge_prompt_4_position = f\"\"\"Which of the two following answers is better?\n",
    "Answer A: '{model_answer_B_4}'\n",
    "Answer B: '{model_answer_A_4}'\"\"\" # Swapped order\n",
    "\n",
    "print(f\"\\nLLM Judge Prompt (B first):\\n{llm_judge_prompt_4_position}\")\n",
    "\n",
    "response_4_position = llm.invoke(llm_judge_prompt_4_position)\n",
    "print(f\"LLM Response (B first): {response_4_position.content}\")\n",
    "\n",
    "print(\"\\n**Hidden flaw (Position Bias, as per your description):** If the order of the answers were swapped (as demonstrated in the second prompt), there is a chance the LLM would favor the new 'Answer A' (which is now the less verbose one), demonstrating an unconscious preference for the first item presented. This bias is particularly problematic for instances where the answers are of similar quality, as the LLM's preference can be swayed by the arbitrary ordering.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Path Forward\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "**The Problem:**\n",
    "1. **Traditional evaluation methods** don't work for modern AI systems\n",
    "2. **AI models fail silently** without proper evaluation\n",
    "3. **Naive LLM judging approaches** have hidden flaws\n",
    "\n",
    "**The Solution:**\n",
    "1. **LLM as Judge** provides scalable, understanding\n",
    "2. **Proper implementation** requires systematic approaches\n",
    "3. **Success measurement** needs concrete metrics and bias detection\n",
    "\n",
    "### Next Steps:\n",
    "In the following sessions, we'll build sophisticated solutions:\n",
    "- **Session 2**: Progressive improvements and structured approaches\n",
    "- **Session 3**: Production-ready systems with bias detection\n",
    "- **Final Challenge**: Building comprehensive evaluation pipelines\n",
    "\n",
    "---\n",
    "\n",
    "**You now understand both the promise and perils of LLM as Judge systems. Ready to build better solutions?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qna_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
