{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.4: Evaluate Your Prompt Templates with LLM-as-Judge\n",
    "\n",
    "**üìç Progress:** Advanced Section (Optional) | ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "| **Aspect** | **Details** |\n",
    "|-------------|-------------|\n",
    "| **Goal** | Add an evaluation layer that scores outputs from your prompt templates before they reach production |\n",
    "| **Time** | ~40 minutes |\n",
    "| **Prerequisites** | Sections 3.1‚Äì3.3 complete, `setup_utils.py` loaded |\n",
    "| **Level** | **Advanced** - Recommended after mastering 3.2 & 3.3 |\n",
    "| **What You'll Strengthen** | Trustworthy automation, rubric design, quality gates |\n",
    "| **Next Steps** | Return to the [Module 3 overview](./README.md) or wire scores into your workflow |\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° New to this module?** This is an **advanced optional section**. If you haven't completed Sections 3.2 and 3.3, go back and master those first. This section builds on that foundation.\n",
    "\n",
    "You just built reusable prompt templates in Sections 3.2 and 3.3. Now you'll learn how to **evaluate those AI outputs** with an LLM-as-Judge so you can accept great responses, request revisions, or escalate risky ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Quick Setup Check\n",
    "\n",
    "Since you completed Section 1, setup is already done! We just need to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick setup check - imports setup_utils\n",
    "try:\n",
    "    import importlib\n",
    "    import setup_utils\n",
    "    importlib.reload(setup_utils)\n",
    "    from setup_utils import *\n",
    "    print(f\"‚úÖ Setup loaded! Using {PROVIDER.upper()} with {get_default_model()}\")\n",
    "    print(\"üöÄ Ready to score AI outputs with an LLM judge!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Setup not found!\")\n",
    "    print(\"üí° Please run 3.1-setup-and-introduction.ipynb first to set up your environment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è LLM-as-Judge Evaluation Template\n",
    "\n",
    "### Building the Evaluation Loop for Your Prompt Templates\n",
    "\n",
    "<div style=\"background:#fef3c7; border-left:4px solid #f59e0b; padding:16px; border-radius:6px; margin:20px 0; color:#000000;\">\n",
    "<strong style=\"color:#92400e;\">üéØ What You'll Build in This Section</strong><br><br>\n",
    "\n",
    "You'll create an **LLM-as-Judge rubric** that reviews the output produced by your prompt templates. The judge scores the response, explains its verdict, and tells you whether to accept it, request a revision, or fall back to a human reviewer.\n",
    "<br><br>\n",
    "<strong>Time Required:</strong> ~25 minutes (learn, see the example, then try it on your own outputs)\n",
    "</div>\n",
    "\n",
    "Layering a judge after your templates keeps quality high without sending everything back to humans. In Session 1 we saw that traditional metrics (F1, BLEU, ROUGE) miss hallucinations and manual reviews are too slow to scale. A rubric-driven LLM judge gives you semantic understanding *and* consistent scoring.\n",
    "\n",
    "#### üéØ The Problem We're Solving\n",
    "\n",
    "1. **üö® Silent Failures**\n",
    "   - Template-generated outputs can look polished while hiding factual or security mistakes.\n",
    "   - Legacy metrics can't flag these issues because they only check surface-level overlap.\n",
    "\n",
    "2. **‚è≥ Manual QA Bottlenecks**\n",
    "   - Human spot checks take days and don't scale to thousands of AI responses.\n",
    "   - Feedback arrives too late to keep CI/CD pipelines moving.\n",
    "\n",
    "3. **üéØ Inconsistent Standards**\n",
    "   - Without a codified rubric, every reviewer (human or AI) applies different criteria.\n",
    "   - Teams struggle to know when to ship, regenerate, or escalate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèóÔ∏è How We'll Build It: The Tactical Combination\n",
    "\n",
    "We chain together Module 2 tactics plus what you learned about judges in Session 1.\n",
    "\n",
    "| **Tactic** | **Purpose in This Template** | **Why Modern LLMs Need This** |\n",
    "|------------|------------------------------|-------------------------------|\n",
    "| **Role Prompting** | Positions the judge as a principal engineer with review authority | Anchors the evaluation in expert expectations instead of generic chat replies |\n",
    "| **Structured Inputs** | Separates context, rubric, and submission using XML-style tags | Prevents the model from blending instructions with the artifact under review |\n",
    "| **Rubric Decomposition** | Breaks quality into weighted criteria | Mirrors Session 1 guidance: multi-dimensional scoring avoids naive pass/fail |\n",
    "| **Chain-of-Thought Justification** | Forces rationale before the decision | Produces auditable feedback and catches hallucinations sooner |\n",
    "| **Decision Thresholds** | Maps weighted score to Accept / Revise / Reject actions | Gives your pipeline a clear automation hook instead of reading prose |\n",
    "\n",
    "<div style=\"margin:16px 0; padding:16px; background:#eef2ff; border-left:5px solid #4338ca; border-radius:8px; color:#1f2937;\">\n",
    "<strong style=\"font-size:1.05em; color:#1e1b4b;\">Reminder from Session 1</strong><br><br>\n",
    "Relying on a single yes/no question (for example, 'Is this output correct?') lets hidden errors slip through. Weighted rubrics with explicit thresholds give you measurable guardrails.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Why Add a Judge After Prompt Templates?\n",
    "\n",
    "- **Detect hidden regressions:** LLM judges evaluate meaning, so paraphrased but wrong answers score poorly even when lexical metrics look fine.\n",
    "- **Keep automation trustworthy:** A second AI call verifies that template outputs meet the same criteria every time, reducing escalation load.\n",
    "- **Accelerate iteration:** Scores highlight which tactic block to tweak, letting you A/B test prompts without waiting for human reviewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã LLM-as-Judge Rubric Template\n",
    "\n",
    "```xml\n",
    "<role>\n",
    "You are a Principal Engineer reviewing AI-generated code feedback.\n",
    "</role>\n",
    "\n",
    "<rubric>\n",
    "1. Accuracy (40%): Do identified issues actually exist and are correctly described?\n",
    "2. Completeness (30%): Are major concerns covered? Any critical issues missed?\n",
    "3. Actionability (20%): Are recommendations specific and implementable?\n",
    "4. Communication (10%): Is the review professional, clear, and well-structured?\n",
    "</rubric>\n",
    "\n",
    "<instructions>\n",
    "Score each criterion 1-5 with detailed rationale:\n",
    "- 5: Excellent - Exceeds expectations\n",
    "- 4: Good - Meets expectations with minor gaps\n",
    "- 3: Acceptable - Meets minimum bar\n",
    "- 2: Needs work - Significant gaps\n",
    "- 1: Unacceptable - Fails to meet standards\n",
    "\n",
    "Calculate weighted total: (Accuracy√ó0.4) + (Completeness√ó0.3) + (Actionability√ó0.2) + (Communication√ó0.1)\n",
    "\n",
    "Recommend:\n",
    "- ACCEPT (‚â•3.5): Production-ready\n",
    "- REVISE (2.5-3.4): Needs improvements, provide specific guidance\n",
    "- REJECT (<2.5): Start over with different approach\n",
    "</instructions>\n",
    "\n",
    "<submission>\n",
    "{{llm_output_under_review}}\n",
    "</submission>\n",
    "\n",
    "<output_format>\n",
    "Provide structured evaluation with:\n",
    "- Individual scores (1-5) with rationale for each criterion\n",
    "- Weighted total score\n",
    "- Recommendation (ACCEPT/REVISE/REJECT)\n",
    "- Specific feedback for improvements\n",
    "</output_format>\n",
    "```\n",
    "\n",
    "#### üîë Rubric Design Principles\n",
    "\n",
    "1. **Weighted Criteria** ‚Äì Prioritise what matters most (accuracy first for safety-critical domains).\n",
    "2. **Explicit Scale** ‚Äì Clear definitions stop the judge from drifting between runs.\n",
    "3. **Evidence-Based Rationale** ‚Äì Forces the model to ground scores in the submission.\n",
    "4. **Actionable Thresholds** ‚Äì Numeric gates let pipelines auto-approve or request revisions.\n",
    "5. **Improvement Guidance** ‚Äì \"Revise\" outcomes must include next steps for the generator.\n",
    "\n",
    "#### üß™ Calibration Framework\n",
    "\n",
    "The rubric above tells the judge **what** to score; calibration makes sure everyone scores it the **same way**. Treat calibration notes as the companion playbook that keeps your accuracy/completeness/actionability/communication scores aligned across reviewers and over time.\n",
    "\n",
    "Instead of generic \"7/10 - pretty good\" language, define what each score means. For example, **7/10 = factually accurate with minor gaps, clear structure, appropriate for the target audience, but missing one or two implementation details.**\n",
    "\n",
    "#### üõ†Ô∏è Use-Case Calibration Examples\n",
    "\n",
    "Tie calibration back to your weighted criteria: the examples below show how different score levels reflect accuracy, completeness, actionability, and communication in a documentation context.\n",
    "\n",
    "| Scenario | 9/10 | 5/10 | 2/10 |\n",
    "|----------|------|------|------|\n",
    "| Technical documentation | Complete, tested, and handles edge cases | Covers main flows, some gaps in error handling | Only basic concepts, missing implementation details |\n",
    "\n",
    "#### üìè Calibration Best Practices\n",
    "\n",
    "- **Anchor scores:** Use real examples for every score level so the judge can compare and map them back to the rubric criteria.\n",
    "- **Regular recalibration:** Review rubrics quarterly with domain experts and adjust thresholds or weights as standards evolve.\n",
    "- **Inter-rater reliability:** Have multiple calibrators score the same samples to confirm they interpret the rubric the same way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üíª Working Example: Judge the Section 3.2 Code Review\n",
    "\n",
    "> **Note:** To avoid the AI model grading its own review or automatically preferring its own output, we switch the judge to a different provider/model so the evaluation comes from an independent model.\n",
    "\n",
    "This cell replays the Section 3.2 template to generate the comprehensive AI review, then immediately scores it with the judge using the same monthly report diff.\n",
    "\n",
    "**What you'll see:**\n",
    "- The full AI review that the template produces\n",
    "- How the rubric weights accuracy, completeness, actionability, and communication\n",
    "- An Accept/Revise/Reject recommendation tied to the numeric thresholds\n",
    "\n",
    "<div style=\"margin-top:16px; padding:16px; background:#fef3c7; border-left:4px solid #f59e0b; border-radius:8px; color:#78350f;\">\n",
    "<strong>‚ö†Ô∏è Heads-up:</strong> <br>\n",
    "The next cell first replays the Section 3.2 prompt template to regenerate the AI review, then runs the LLM-as-Judge rubric on that fresh output.\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top:16px; color:#991b1b; padding:12px; background:#fee2e2; border-radius:6px; border-left:4px solid #ef4444;\">\n",
    "<strong>‚ö†Ô∏è IMPORTANT:</strong><br>\n",
    "To avoid the AI model grading its own review or automatically preferring its own output, we switch the judge to a different provider/model so the evaluation comes from an independent model.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Judge the Section 3.2 code review output\n",
    "\n",
    "code_diff = '''\n",
    "+ import json\n",
    "+ import time\n",
    "+ from decimal import Decimal\n",
    "+\n",
    "+ CACHE = {}\n",
    "+\n",
    "+ def generate_monthly_report(org_id, db, s3_client):\n",
    "+     if org_id in CACHE:\n",
    "+         return CACHE[org_id]\n",
    "+\n",
    "+     query = f\"SELECT * FROM invoices WHERE org_id = '{org_id}' ORDER BY created_at DESC\"\n",
    "+     rows = db.execute(query)\n",
    "+\n",
    "+     total = Decimal(0)\n",
    "+     items = []\n",
    "+     for row in rows:\n",
    "+         total += Decimal(row['amount'])\n",
    "+         items.append({\n",
    "+             'id': row['id'],\n",
    "+             'customer': row['customer_name'],\n",
    "+             'amount': float(row['amount'])\n",
    "+         })\n",
    "+\n",
    "+     payload = {\n",
    "+         'org': org_id,\n",
    "+         'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "+         'total': float(total),\n",
    "+         'items': items\n",
    "+     }\n",
    "+\n",
    "+     key = f\"reports/{org_id}/{int(time.time())}.json\"\n",
    "+     time.sleep(0.5)\n",
    "+     s3_client.put_object(\n",
    "+         Bucket='company-reports',\n",
    "+         Key=key,\n",
    "+         Body=json.dumps(payload),\n",
    "+         ACL='public-read'\n",
    "+     )\n",
    "+\n",
    "+     CACHE[org_id] = key\n",
    "+     return key\n",
    "'''\n",
    "\n",
    "review_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You follow structured review templates and produce clear, actionable findings.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "<role>\n",
    "Act as a Senior Software Engineer specializing in Python backend services.\n",
    "Your expertise covers security best practices, performance tuning, reliability, and maintainable design.\n",
    "</role>\n",
    "\n",
    "<context>\n",
    "Repository: analytics-platform\n",
    "Service: Reporting API\n",
    "Purpose: Add a monthly invoice report exporter that finance can trigger\n",
    "Change Scope: Review focuses on the generate_monthly_report implementation\n",
    "Language: python\n",
    "</context>\n",
    "\n",
    "<code_diff>\n",
    "{code_diff}\n",
    "</code_diff>\n",
    "\n",
    "<review_guidelines>\n",
    "Assess the change across multiple dimensions:\n",
    "\n",
    "1. Security ‚Äî SQL injection, S3 object exposure, sensitive data handling.\n",
    "2. Performance ‚Äî query efficiency, blocking calls, caching behaviour.\n",
    "3. Error Handling ‚Äî resilience to empty results, network/storage failures.\n",
    "4. Code Quality ‚Äî readability, global state, data conversions.\n",
    "5. Correctness ‚Äî totals, currency precision, repeated report generation.\n",
    "6. Best Practices ‚Äî configuration management, separation of concerns, testing hooks.\n",
    "For each finding, cite the diff line, describe impact, and share an actionable fix.\n",
    "</review_guidelines>\n",
    "\n",
    "<tasks>\n",
    "Step 1 - Think: Analyse the diff using the dimensions listed above.\n",
    "Step 2 - Assess: For each issue, capture Severity (CRITICAL/MAJOR/MINOR/INFO), Category, Line, Issue, Impact.\n",
    "Step 3 - Suggest: Provide a concrete remediation (code change or process tweak).\n",
    "Step 4 - Verdict: Summarise overall risk and recommend APPROVE / REQUEST CHANGES / NEEDS WORK.\n",
    "</tasks>\n",
    "\n",
    "<output_format>\n",
    "## Code Review Summary\n",
    "[One paragraph on overall health and primary risks]\n",
    "\n",
    "## Findings\n",
    "### [SEVERITY] Issue Title\n",
    "**Category:** [Security / Performance / Quality / Correctness / Best Practices]\n",
    "**Line:** [line number]\n",
    "**Issue:** [impact-focused description]\n",
    "**Recommendation:**\n",
    "```\n",
    "# safer / faster / cleaner fix here\n",
    "```\n",
    "\n",
    "## Overall Assessment\n",
    "**Recommendation:** [APPROVE | REQUEST CHANGES | NEEDS WORK]\n",
    "**Summary:** [What to address before merge]\n",
    "</output_format>\n",
    "\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"üîç Generating the Section 3.2 code review...\")\n",
    "print(f\"Using {PROVIDER.upper()} with {get_default_model()}\")\n",
    "print(\"=\" * 70)\n",
    "ai_generated_review = get_chat_completion(review_messages, temperature=0.0)\n",
    "print(ai_generated_review)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rubric_prompt = \"\"\"\n",
    "<context>\n",
    "Original pull request diff:\n",
    "{context}\n",
    "\n",
    "AI-generated review to evaluate:\n",
    "{ai_output}\n",
    "</context>\n",
    "\n",
    "<rubric>\n",
    "1. Accuracy (40%): Do identified issues actually exist and are correctly described?\n",
    "2. Completeness (30%): Are major concerns covered? Any critical issues missed?\n",
    "3. Actionability (20%): Are recommendations specific and implementable?\n",
    "4. Communication (10%): Is the review professional, clear, and well-structured?\n",
    "</rubric>\n",
    "\n",
    "<instructions>\n",
    "Score each criterion 1-5 with detailed rationale.\n",
    "Calculate weighted total: (Accuracy√ó0.4) + (Completeness√ó0.3) + (Actionability√ó0.2) + (Communication√ó0.1)\n",
    "\n",
    "Recommend:\n",
    "- ACCEPT (‚â•3.5): Production-ready\n",
    "- REVISE (2.5-3.4): Needs improvements  \n",
    "- REJECT (<2.5): Unacceptable quality\n",
    "</instructions>\n",
    "\n",
    "Provide structured evaluation with scores, weighted total, recommendation, and feedback.\n",
    "\"\"\"\n",
    "\n",
    "judge_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Principal Engineer reviewing AI-generated code feedback.\"},\n",
    "    {\"role\": \"user\", \"content\": rubric_prompt.format(context=code_diff, ai_output=ai_generated_review)}\n",
    "]\n",
    "\n",
    "original_provider = setup_utils.PROVIDER\n",
    "try:\n",
    "    setup_utils.PROVIDER = 'openai'\n",
    "    print(\"‚öñÔ∏è JUDGE EVALUATION IN PROGRESS...\")\n",
    "    print(f\"Using {PROVIDER.upper()} with {get_default_model()}\")\n",
    "    print(\"=\" * 70)\n",
    "    judge_result = get_chat_completion(judge_messages, temperature=0.0)\n",
    "    print(judge_result)\n",
    "    print(\"=\" * 70)\n",
    "finally:\n",
    "    setup_utils.PROVIDER = original_provider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Activity 3.4: Create Your Judge Template\n",
    "\n",
    "**Now it's your turn!** Complete Activity 3.4 to build your own judge template.\n",
    "\n",
    "### üìù Instructions\n",
    "\n",
    "1. **Open the activity file**: `activities/activity-3.4-llm-as-judge-evaluation.md`\n",
    "2. **Edit the template**: Replace all `TODO` comments with your scoring criteria\n",
    "3. **Come back here**: Run the cells below to test your template\n",
    "4. **Iterate**: Refine your template based on the results\n",
    "\n",
    "**What you're building**: A judge that evaluates the cache refactor scenario against 4 weighted criteria (correctness, design, safety, tests) and outputs Accept/Revise/Block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top:16px; color:#991b1b; padding:12px; background:#fee2e2; border-radius:6px; border-left:4px solid #ef4444;\">\n",
    "<style>\n",
    "code {\n",
    "  font-family: Consolas,\"courier new\";\n",
    "  color:rgb(238, 13, 13);\n",
    "  background-color: #f1f1f1;\n",
    "  padding: 2px;\n",
    "  font-size: 110%;\n",
    "}\n",
    "</style>\n",
    "<strong>‚ö†Ô∏è COMPLETE THE ACTIVITY FIRST:</strong><br>\n",
    "Before running the cells below, you must:\n",
    "<ol style=\"margin: 8px 0 0 0;\">\n",
    "<li>Open <code>activities/activity-3.4-llm-as-judge-evaluation.md</code></li>\n",
    "<li>Replace all <code>TODO</code> comments in the template (between <code>&lt;!-- TEMPLATE START --&gt;</code> and <code>&lt;!-- TEMPLATE END --&gt;</code>)</li>\n",
    "<li>Save the file</li>\n",
    "<li>Return here to test your template</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top:16px; color:#78350f; padding:12px; background:#fef3c7; border-radius:6px; border-left:4px solid #f59e0b;\">\n",
    "<strong>üí° What the test provides:</strong><br>\n",
    "The <code>test_activity_3_4()</code> function loads your template and fills in all the scenario details (code_before, code_after, refactor rationale, etc.). You just need to define the scoring criteria!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÅ Test Your Judge Template\n",
    "\n",
    "Run the cell below to test your completed template. This loads your template from the activity file and evaluates the cache refactor scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your judge template with the cache refactor scenario\n",
    "from setup_utils import test_activity_3_4, get_refactor_judge_scenario\n",
    "\n",
    "print(\"üß™ Testing your judge template from activity-3.4-llm-as-judge-evaluation.md...\")\n",
    "print(\"=\" * 70)\n",
    "judge_preview = test_activity_3_4(variables=get_refactor_judge_scenario())\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüí° Review the verdict above. Does it match your expectations?\")\n",
    "print(\"   - If TODOs remain, complete your template in the activity file\")\n",
    "print(\"   - If scores seem off, adjust your criteria and re-run this cell\")\n",
    "print(\"   - To see the reference solution, check solutions/activity-3.4-judge-solution.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test with custom scenario\n",
    "# \n",
    "# If you want to test your judge with different code, modify the variables below\n",
    "# and run this cell. Otherwise, the cell above tests with the standard scenario.\n",
    "\n",
    "from setup_utils import test_activity_3_4\n",
    "\n",
    "custom_variables = {\n",
    "    'service_name': 'TODO - Your Service Name',\n",
    "    'refactor_brief': 'TODO - What was refactored?',\n",
    "    'code_before': \"\"\"\n",
    "# TODO: Paste original code here\n",
    "\"\"\",\n",
    "    'code_after': \"\"\"\n",
    "# TODO: Paste refactored code here\n",
    "\"\"\",\n",
    "    'refactor_goal': 'TODO - What was the goal?',\n",
    "    'test_summary': 'TODO - Test results',\n",
    "    'analysis_findings': 'TODO - Linter/static analysis output',\n",
    "    'critical_regression': 'TODO - Any known regression?',\n",
    "    'security_findings': 'TODO - Security scan results',\n",
    "    'escalation_channel': '#your-channel',\n",
    "    'ai_refactor_output': \"\"\"\n",
    "# TODO: Paste the AI's explanation of the refactor\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"üß™ Testing with custom scenario...\")\n",
    "print(\"‚ö†Ô∏è Make sure to replace all TODO values above before running!\")\n",
    "print(\"=\" * 70)\n",
    "judge_result = test_activity_3_4(variables=custom_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### üöÄ What's Next: From Manual Judging to Systematic Evals\n",
    "\n",
    "**You just tested your judge on one scenario.** To use this in production, you need **systematic evaluations** that track judge performance over time.\n",
    "\n",
    "#### Why Evals Matter\n",
    "\n",
    "Manual testing validates one case. **Evals** validate your judge across hundreds of cases and track metrics:\n",
    "- **Accuracy**: Does your judge correctly identify good vs bad refactors?\n",
    "- **False positives**: How often does it block acceptable changes?\n",
    "- **Consistency**: Does rubric v2 improve on v1?\n",
    "\n",
    "**Learn why evals are critical:** [Why LLM Evals Matter](https://www.youtube.com/watch?v=vygFgCNR7WA&list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S)\n",
    "\n",
    "#### Production Eval Platforms\n",
    "\n",
    "Scale your judge with evaluation platforms:\n",
    "\n",
    "- **[OpenAI Platform Evals](https://platform.openai.com/docs/guides/evals)**: Dashboard-based systematic evaluation with datasets and metrics\n",
    "- **[Anthropic Evaluation Tool](https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool)**: Console-based prompt testing with side-by-side comparison\n",
    "\n",
    "#### Quick Start\n",
    "\n",
    "1. **Build an eval dataset**: Collect 10-20 refactors with known verdicts\n",
    "2. **Run systematic evals**: Test your judge template against the dataset\n",
    "3. **Track metrics**: Measure accuracy, iterate on rubric weights\n",
    "4. **Compare models**: Test if GPT-4o vs Claude performs better as judge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Learn More: Production-Ready Evaluation Patterns\n",
    "\n",
    "- [LLM as a Judge: Scaling AI Evaluation Strategies](https://youtu.be/trfUBIDeI1Y?si=mxwrME9l3KcpZNPj) - How LLM as a judge can scale and refine evaluations with strategies like direct assessment and pairwise comparison.\n",
    "- [LLM-as-a-Judge: Rethinking Model-Based Evaluations in Text Generation](https://leehanchung.github.io/blogs/2024/08/11/llm-as-a-judge/) - Analyzes the evolution of text generation evaluation methods, from traditional approaches to LLM-as-a-Judge\n",
    "- [The challenges in using LLM-as-a-Judge](https://youtu.be/vBJF2sy1Pyw?si=S5IsgIOu0dzASUbH) - Learn how to use LLM-based evaluations effectively, understand associated challenges and look at what lies beyond evaluation.\n",
    "- [LLM-as-a-judge on Amazon Bedrock Model Evaluation](https://aws.amazon.com/blogs/machine-learning/llm-as-a-judge-on-amazon-bedrock-model-evaluation/) - How to implement and use LLM-as-a-judge capability within Amazon Bedrock Model Evaluation\n",
    "- [Anthropic: Using the Evaluation Tool](https://docs.claude.com/en/docs/test-and-evaluate/eval-tool) - Evaluate prompts in the developer console.\n",
    "- [OpenAI Docs: Graders](https://platform.openai.com/docs/guides/graders) - Graders are a way to evaluate your model's performance against reference answers.\n",
    "- [OpenAI DevDay 2024 | Balancing accuracy, latency, and cost at scale](https://youtu.be/Bx6sUDRMx-8) ‚Äî LLM optimization\n",
    "- [Session 1 Recap](../../session_1_introduction_and_basics.ipynb) - Revisit why automated metrics alone miss hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Section 3.4 Complete!\n",
    "\n",
    "<div style=\"margin-top:16px; padding:14px; background:#dcfce7; border-left:4px solid #22c55e; border-radius:6px; color:#065f46;\">\n",
    "<strong>üéâ Outstanding work!</strong> You've completed the advanced LLM-as-Judge section and mastered all of Module 3!\n",
    "</div>\n",
    "\n",
    "**Key takeaways:**\n",
    "- Built weighted rubrics to evaluate AI-generated outputs\n",
    "- Learned to set automated decision thresholds (Accept/Revise/Block)\n",
    "- Discovered how to scale from manual testing to systematic evals\n",
    "\n",
    "### üéä Module 3 Complete!\n",
    "\n",
    "You've now mastered:\n",
    "- ‚úÖ **Code Review Automation** (Section 3.2)\n",
    "- ‚úÖ **Test Generation Automation** (Section 3.3)\n",
    "- ‚úÖ **LLM-as-Judge Evaluation** (Section 3.4)\n",
    "\n",
    "### ‚è≠Ô∏è Next Steps\n",
    "\n",
    "**Ready to integrate?** Continue to **Module 4: Integration** to learn how to:\n",
    "- Integrate your templates into GitHub Copilot, OpenAI Codex, and Claude Code\n",
    "- Build custom commands and workflows for AI code assistants\n",
    "- Operationalize prompt engineering across your team\n",
    "\n",
    "**Want to apply what you learned?**\n",
    "- Use your judge on Activities 3.2 and 3.3 outputs\n",
    "- Build eval datasets to track judge performance over time\n",
    "- Integrate quality gates into your CI/CD pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
