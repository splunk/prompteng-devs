{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.4: Evaluate Your Prompt Templates with LLM-as-Judge\n",
    "\n",
    "| **Aspect** | **Details** |\n",
    "|-------------|-------------|\n",
    "| **Goal** | Add an evaluation layer that scores outputs from your prompt templates before they reach production |\n",
    "| **Time** | ~25 minutes |\n",
    "| **Prerequisites** | Sections 3.1‚Äì3.3 complete, `setup_utils.py` loaded |\n",
    "| **What You'll Strengthen** | Trustworthy automation, rubric design, quality gates |\n",
    "| **Next Steps** | Return to the [Module 3 overview](./README.md) or wire scores into your workflow |\n",
    "\n",
    "---\n",
    "\n",
    "You just built reusable prompt templates in Sections 3.2 and 3.3. Now you'll learn how to **evaluate those AI outputs** with an LLM-as-Judge so you can accept great responses, request revisions, or escalate risky ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Quick Setup Check\n",
    "\n",
    "Since you completed Section 1, setup is already done! We just need to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick setup check - imports setup_utils\n",
    "try:\n",
    "    import importlib\n",
    "    import setup_utils\n",
    "    importlib.reload(setup_utils)\n",
    "    from setup_utils import *\n",
    "    print(f\"‚úÖ Setup loaded! Using {PROVIDER.upper()} with {get_default_model()}\")\n",
    "    print(\"üöÄ Ready to score AI outputs with an LLM judge!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Setup not found!\")\n",
    "    print(\"üí° Please run 3.1-setup-and-introduction.ipynb first to set up your environment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è LLM-as-Judge Evaluation Template\n",
    "\n",
    "### Building the Evaluation Loop for Your Prompt Templates\n",
    "\n",
    "<div style=\"background:#fef3c7; border-left:4px solid #f59e0b; padding:16px; border-radius:6px; margin:20px 0; color:#000000;\">\n",
    "<strong style=\"color:#92400e;\">üéØ What You'll Build in This Section</strong><br><br>\n",
    "\n",
    "You'll create an **LLM-as-Judge rubric** that reviews the output produced by your prompt templates. The judge scores the response, explains its verdict, and tells you whether to accept it, request a revision, or fall back to a human reviewer.\n",
    "<br><br>\n",
    "<strong>Time Required:</strong> ~25 minutes (learn, see the example, then try it on your own outputs)\n",
    "</div>\n",
    "\n",
    "Layering a judge after your templates keeps quality high without sending everything back to humans. In Session 1 we saw that traditional metrics (F1, BLEU, ROUGE) miss hallucinations and manual reviews are too slow to scale. A rubric-driven LLM judge gives you semantic understanding *and* consistent scoring.\n",
    "\n",
    "#### üéØ The Problem We're Solving\n",
    "\n",
    "1. **üö® Silent Failures**\n",
    "   - Template-generated outputs can look polished while hiding factual or security mistakes.\n",
    "   - Legacy metrics can't flag these issues because they only check surface-level overlap.\n",
    "\n",
    "2. **‚è≥ Manual QA Bottlenecks**\n",
    "   - Human spot checks take days and don't scale to thousands of AI responses.\n",
    "   - Feedback arrives too late to keep CI/CD pipelines moving.\n",
    "\n",
    "3. **üéØ Inconsistent Standards**\n",
    "   - Without a codified rubric, every reviewer (human or AI) applies different criteria.\n",
    "   - Teams struggle to know when to ship, regenerate, or escalate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèóÔ∏è How We'll Build It: The Tactical Combination\n",
    "\n",
    "We chain together Module 2 tactics plus what you learned about judges in Session 1.\n",
    "\n",
    "| **Tactic** | **Purpose in This Template** | **Why Modern LLMs Need This** |\n",
    "|------------|------------------------------|-------------------------------|\n",
    "| **Role Prompting** | Positions the judge as a principal engineer with review authority | Anchors the evaluation in expert expectations instead of generic chat replies |\n",
    "| **Structured Inputs** | Separates context, rubric, and submission using XML-style tags | Prevents the model from blending instructions with the artifact under review |\n",
    "| **Rubric Decomposition** | Breaks quality into weighted criteria | Mirrors Session 1 guidance: multi-dimensional scoring avoids naive pass/fail |\n",
    "| **Chain-of-Thought Justification** | Forces rationale before the decision | Produces auditable feedback and catches hallucinations sooner |\n",
    "| **Decision Thresholds** | Maps weighted score to Accept / Revise / Reject actions | Gives your pipeline a clear automation hook instead of reading prose |\n",
    "\n",
    "<div style=\"margin:16px 0; padding:16px; background:#eef2ff; border-left:5px solid #4338ca; border-radius:8px; color:#1f2937;\">\n",
    "<strong style=\"font-size:1.05em; color:#1e1b4b;\">Reminder from Session 1</strong><br><br>\n",
    "Relying on a single yes/no question (for example, 'Is this output correct?') lets hidden errors slip through. Weighted rubrics with explicit thresholds give you measurable guardrails.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Why Add a Judge After Prompt Templates?\n",
    "\n",
    "- **Detect hidden regressions:** LLM judges evaluate meaning, so paraphrased but wrong answers score poorly even when lexical metrics look fine.\n",
    "- **Keep automation trustworthy:** A second AI call verifies that template outputs meet the same criteria every time, reducing escalation load.\n",
    "- **Accelerate iteration:** Scores highlight which tactic block to tweak, letting you A/B test prompts without waiting for human reviewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã LLM-as-Judge Rubric Template\n",
    "\n",
    "```xml\n",
    "<role>\n",
    "You are a Principal Engineer reviewing AI-generated code feedback.\n",
    "</role>\n",
    "\n",
    "<rubric>\n",
    "1. Accuracy (40%): Do identified issues actually exist and are correctly described?\n",
    "2. Completeness (30%): Are major concerns covered? Any critical issues missed?\n",
    "3. Actionability (20%): Are recommendations specific and implementable?\n",
    "4. Communication (10%): Is the review professional, clear, and well-structured?\n",
    "</rubric>\n",
    "\n",
    "<instructions>\n",
    "Score each criterion 1-5 with detailed rationale:\n",
    "- 5: Excellent - Exceeds expectations\n",
    "- 4: Good - Meets expectations with minor gaps\n",
    "- 3: Acceptable - Meets minimum bar\n",
    "- 2: Needs work - Significant gaps\n",
    "- 1: Unacceptable - Fails to meet standards\n",
    "\n",
    "Calculate weighted total: (Accuracy√ó0.4) + (Completeness√ó0.3) + (Actionability√ó0.2) + (Communication√ó0.1)\n",
    "\n",
    "Recommend:\n",
    "- ACCEPT (‚â•3.5): Production-ready\n",
    "- REVISE (2.5-3.4): Needs improvements, provide specific guidance\n",
    "- REJECT (<2.5): Start over with different approach\n",
    "</instructions>\n",
    "\n",
    "<submission>\n",
    "{{llm_output_under_review}}\n",
    "</submission>\n",
    "\n",
    "<output_format>\n",
    "Provide structured evaluation with:\n",
    "- Individual scores (1-5) with rationale for each criterion\n",
    "- Weighted total score\n",
    "- Recommendation (ACCEPT/REVISE/REJECT)\n",
    "- Specific feedback for improvements\n",
    "</output_format>\n",
    "```\n",
    "\n",
    "#### üîë Rubric Design Principles\n",
    "\n",
    "1. **Weighted Criteria** ‚Äì Prioritise what matters most (accuracy first for safety-critical domains).\n",
    "2. **Explicit Scale** ‚Äì Clear definitions stop the judge from drifting between runs.\n",
    "3. **Evidence-Based Rationale** ‚Äì Forces the model to ground scores in the submission.\n",
    "4. **Actionable Thresholds** ‚Äì Numeric gates let pipelines auto-approve or request revisions.\n",
    "5. **Improvement Guidance** ‚Äì \"Revise\" outcomes must include next steps for the generator.\n",
    "\n",
    "#### üß™ Calibration Framework\n",
    "\n",
    "The rubric above tells the judge **what** to score; calibration makes sure everyone scores it the **same way**. Treat calibration notes as the companion playbook that keeps your accuracy/completeness/actionability/communication scores aligned across reviewers and over time.\n",
    "\n",
    "Instead of generic \"7/10 - pretty good\" language, define what each score means. For example, **7/10 = factually accurate with minor gaps, clear structure, appropriate for the target audience, but missing one or two implementation details.**\n",
    "\n",
    "#### üõ†Ô∏è Use-Case Calibration Examples\n",
    "\n",
    "Tie calibration back to your weighted criteria: the examples below show how different score levels reflect accuracy, completeness, actionability, and communication in a documentation context.\n",
    "\n",
    "| Scenario | 9/10 | 5/10 | 2/10 |\n",
    "|----------|------|------|------|\n",
    "| Technical documentation | Complete, tested, and handles edge cases | Covers main flows, some gaps in error handling | Only basic concepts, missing implementation details |\n",
    "\n",
    "#### üìè Calibration Best Practices\n",
    "\n",
    "- **Anchor scores:** Use real examples for every score level so the judge can compare and map them back to the rubric criteria.\n",
    "- **Regular recalibration:** Review rubrics quarterly with domain experts and adjust thresholds or weights as standards evolve.\n",
    "- **Inter-rater reliability:** Have multiple calibrators score the same samples to confirm they interpret the rubric the same way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üíª Working Example: Judge the Section 3.2 Code Review\n",
    "\n",
    "This cell replays the Section 3.2 template to generate the comprehensive AI review, then immediately scores it with the judge using the same monthly report diff.\n",
    "\n",
    "**What you'll see:**\n",
    "- The full AI review that the template produces\n",
    "- How the rubric weights accuracy, completeness, actionability, and communication\n",
    "- An Accept/Revise/Reject recommendation tied to the numeric thresholds\n",
    "\n",
    "<div style=\"margin-top:16px; padding:16px; background:#fef3c7; border-left:4px solid #f59e0b; border-radius:8px; color:#78350f;\">\n",
    "<strong>‚ö†Ô∏è Heads-up:</strong> <br><br>\n",
    "The next cell first replays the Section 3.2 prompt template to regenerate the AI review, then runs the LLM-as-Judge rubric on that fresh output.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Judge the Section 3.2 code review output\n",
    "\n",
    "code_diff = '''\n",
    "+ import json\n",
    "+ import time\n",
    "+ from decimal import Decimal\n",
    "+\n",
    "+ CACHE = {}\n",
    "+\n",
    "+ def generate_monthly_report(org_id, db, s3_client):\n",
    "+     if org_id in CACHE:\n",
    "+         return CACHE[org_id]\n",
    "+\n",
    "+     query = f\"SELECT * FROM invoices WHERE org_id = '{org_id}' ORDER BY created_at DESC\"\n",
    "+     rows = db.execute(query)\n",
    "+\n",
    "+     total = Decimal(0)\n",
    "+     items = []\n",
    "+     for row in rows:\n",
    "+         total += Decimal(row['amount'])\n",
    "+         items.append({\n",
    "+             'id': row['id'],\n",
    "+             'customer': row['customer_name'],\n",
    "+             'amount': float(row['amount'])\n",
    "+         })\n",
    "+\n",
    "+     payload = {\n",
    "+         'org': org_id,\n",
    "+         'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "+         'total': float(total),\n",
    "+         'items': items\n",
    "+     }\n",
    "+\n",
    "+     key = f\"reports/{org_id}/{int(time.time())}.json\"\n",
    "+     time.sleep(0.5)\n",
    "+     s3_client.put_object(\n",
    "+         Bucket='company-reports',\n",
    "+         Key=key,\n",
    "+         Body=json.dumps(payload),\n",
    "+         ACL='public-read'\n",
    "+     )\n",
    "+\n",
    "+     CACHE[org_id] = key\n",
    "+     return key\n",
    "'''\n",
    "\n",
    "review_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You follow structured review templates and produce clear, actionable findings.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "<!-- Block 1: Role -->\n",
    "<role>\n",
    "Act as a Senior Software Engineer specializing in Python backend services.\n",
    "Your expertise covers security best practices, performance tuning, reliability, and maintainable design.\n",
    "</role>\n",
    "\n",
    "<!-- Block 2: Context -->\n",
    "<context>\n",
    "Repository: analytics-platform\n",
    "Service: Reporting API\n",
    "Purpose: Add a monthly invoice report exporter that finance can trigger\n",
    "Change Scope: Review focuses on the generate_monthly_report implementation\n",
    "Language: python\n",
    "</context>\n",
    "\n",
    "<!-- Block 3: Code Diff -->\n",
    "<code_diff>\n",
    "{code_diff}\n",
    "</code_diff>\n",
    "\n",
    "<!-- Block 4: Review Guidelines -->\n",
    "<review_guidelines>\n",
    "Assess the change across multiple dimensions:\n",
    "\n",
    "1. Security ‚Äî SQL injection, S3 object exposure, sensitive data handling.\n",
    "2. Performance ‚Äî query efficiency, blocking calls, caching behaviour.\n",
    "3. Error Handling ‚Äî resilience to empty results, network/storage failures.\n",
    "4. Code Quality ‚Äî readability, global state, data conversions.\n",
    "5. Correctness ‚Äî totals, currency precision, repeated report generation.\n",
    "6. Best Practices ‚Äî configuration management, separation of concerns, testing hooks.\n",
    "For each finding, cite the diff line, describe impact, and share an actionable fix.\n",
    "</review_guidelines>\n",
    "\n",
    "<!-- Block 5: Tasks -->\n",
    "<tasks>\n",
    "Step 1 - Think: Analyse the diff using the dimensions listed above.\n",
    "Step 2 - Assess: For each issue, capture Severity (CRITICAL/MAJOR/MINOR/INFO), Category, Line, Issue, Impact.\n",
    "Step 3 - Suggest: Provide a concrete remediation (code change or process tweak).\n",
    "Step 4 - Verdict: Summarise overall risk and recommend APPROVE / REQUEST CHANGES / NEEDS WORK.\n",
    "</tasks>\n",
    "\n",
    "<!-- Block 6: Output Format -->\n",
    "<output_format>\n",
    "## Code Review Summary\n",
    "[One paragraph on overall health and primary risks]\n",
    "\n",
    "## Findings\n",
    "### [SEVERITY] Issue Title\n",
    "**Category:** [Security / Performance / Quality / Correctness / Best Practices]\n",
    "**Line:** [line number]\n",
    "**Issue:** [impact-focused description]\n",
    "**Recommendation:**\n",
    "```\n",
    "# safer / faster / cleaner fix here\n",
    "```\n",
    "\n",
    "## Overall Assessment\n",
    "**Recommendation:** [APPROVE | REQUEST CHANGES | NEEDS WORK]\n",
    "**Summary:** [What to address before merge]\n",
    "</output_format>\n",
    "\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"üîç Generating the Section 3.2 code review...\")\n",
    "print(\"=\" * 70)\n",
    "ai_generated_review = get_chat_completion(review_messages, temperature=0.0)\n",
    "print(ai_generated_review)\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rubric_prompt = \"\"\"\n",
    "<context>\n",
    "Original pull request diff:\n",
    "{context}\n",
    "\n",
    "AI-generated review to evaluate:\n",
    "{ai_output}\n",
    "</context>\n",
    "\n",
    "<rubric>\n",
    "1. Accuracy (40%): Do identified issues actually exist and are correctly described?\n",
    "2. Completeness (30%): Are major concerns covered? Any critical issues missed?\n",
    "3. Actionability (20%): Are recommendations specific and implementable?\n",
    "4. Communication (10%): Is the review professional, clear, and well-structured?\n",
    "</rubric>\n",
    "\n",
    "<instructions>\n",
    "Score each criterion 1-5 with detailed rationale.\n",
    "Calculate weighted total: (Accuracy√ó0.4) + (Completeness√ó0.3) + (Actionability√ó0.2) + (Communication√ó0.1)\n",
    "\n",
    "Recommend:\n",
    "- ACCEPT (‚â•3.5): Production-ready\n",
    "- REVISE (2.5-3.4): Needs improvements  \n",
    "- REJECT (<2.5): Unacceptable quality\n",
    "</instructions>\n",
    "\n",
    "Provide structured evaluation with scores, weighted total, recommendation, and feedback.\n",
    "\"\"\"\n",
    "\n",
    "judge_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Principal Engineer reviewing AI-generated code feedback.\"},\n",
    "    {\"role\": \"user\", \"content\": rubric_prompt.format(context=code_diff, ai_output=ai_generated_review)}\n",
    "]\n",
    "\n",
    "print(\"‚öñÔ∏è JUDGE EVALUATION IN PROGRESS...\")\n",
    "print(\"=\" * 70)\n",
    "judge_result = get_chat_completion(judge_messages, temperature=0.0)\n",
    "print(judge_result)\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Hands-On Practice: Evaluate Your Templates\n",
    "\n",
    "Use the judge to score the outputs you generated in Activities 3.1 and 3.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top:16px; color:#991b1b; padding:12px; background:#fee2e2; border-radius:6px; border-left:4px solid #ef4444;\">\n",
    "<strong>‚ö†Ô∏è IMPORTANT:</strong> Capture the AI output you want to judge before running the cell below.\n",
    "<br><br>\n",
    "<strong>Steps to complete first:</strong>\n",
    "<ul style=\"margin: 8px 0 0 0;\">\n",
    "<li>Run <code>test_activity_3_2(...)</code> or <code>test_activity_3_3(...)</code> to generate the AI response from your template.</li>\n",
    "<li>Save the original artifact (code snippet, ticket, or plan) that the AI evaluated.</li>\n",
    "<li>Copy the AI response into the <code>ai_output_under_review</code> placeholder in the next cell.</li>\n",
    "<li>Optional: Store the judge score alongside your activity file for future comparisons.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top:16px; color:#78350f; padding:12px; background:#fef3c7; border-radius:6px; border-left:4px solid #f59e0b;\">\n",
    "<strong>üí° Tip:</strong> Run the judge after every major template change. Tracking the scores over time makes regressions obvious and keeps your automation trustworthy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score your own AI output with the judge\n",
    "\n",
    "artifact_context = \"\"\"\n",
    "# Paste the original artifact here (code snippet, ticket, requirement, etc.)\n",
    "\"\"\"\n",
    "\n",
    "ai_output_under_review = \"\"\"\n",
    "# Paste the AI-generated review or plan you want to evaluate\n",
    "\"\"\"\n",
    "\n",
    "rubric_prompt = \"\"\"\n",
    "<context>\n",
    "Original artifact:\n",
    "{context}\n",
    "\n",
    "AI-generated output to evaluate:\n",
    "{ai_output}\n",
    "</context>\n",
    "\n",
    "<rubric>\n",
    "1. Accuracy (40%): Do identified issues actually exist and are correctly described?\n",
    "2. Completeness (30%): Are major concerns covered? Any critical issues missed?\n",
    "3. Actionability (20%): Are recommendations specific and implementable?\n",
    "4. Communication (10%): Is the review professional, clear, and well-structured?\n",
    "</rubric>\n",
    "\n",
    "<instructions>\n",
    "Score each criterion 1-5 with detailed rationale.\n",
    "Calculate weighted total: (Accuracy√ó0.4) + (Completeness√ó0.3) + (Actionability√ó0.2) + (Communication√ó0.1)\n",
    "\n",
    "Recommend:\n",
    "- ACCEPT (‚â•3.5): Production-ready\n",
    "- REVISE (2.5-3.4): Needs improvements  \n",
    "- REJECT (<2.5): Unacceptable quality\n",
    "</instructions>\n",
    "\n",
    "Provide structured evaluation with scores, weighted total, recommendation, and feedback.\n",
    "\"\"\"\n",
    "\n",
    "def run_judge_evaluation(context, ai_output, temp=0.0):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a Principal Engineer reviewing AI-generated code feedback.\"},\n",
    "        {\"role\": \"user\", \"content\": rubric_prompt.format(context=context, ai_output=ai_output)}\n",
    "    ]\n",
    "    print(\"‚öñÔ∏è JUDGE EVALUATION IN PROGRESS...\")\n",
    "    print(\"=\" * 70)\n",
    "    result = get_chat_completion(messages, temperature=temp)\n",
    "    print(result)\n",
    "    print(\"=\" * 70)\n",
    "    return result\n",
    "\n",
    "if artifact_context.strip() and ai_output_under_review.strip():\n",
    "    run_judge_evaluation(artifact_context, ai_output_under_review)\n",
    "else:\n",
    "    print(\"‚úã Add your artifact and AI output above before running the judge.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö Learn More: Production-Ready Evaluation Patterns\n",
    "\n",
    "- [Anthropic: Claude Prompting & Evaluation](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/evaluating-outputs) ‚Äî Advanced rubric techniques and bias checks.\n",
    "- [OpenAI Cookbook: Model Grading Patterns](https://cookbook.openai.com/examples/evals/model-graded-eval) ‚Äî How to structure model-graded evaluations and plug them into CI.\n",
    "- [Weights & Biases Evaluations Guide](https://docs.wandb.ai/guides/llm-evaluations) ‚Äî Capture judge scores alongside offline experiments.\n",
    "- [Session 1 Recap](../../session_1_introduction_and_basics.ipynb) ‚Äî Revisit why automated metrics alone miss hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Section 4 Complete!\n",
    "\n",
    "<div style=\"margin-top:16px; padding:14px; background:#dcfce7; border-left:4px solid #22c55e; border-radius:6px; color:#065f46;\">\n",
    "<strong>üéâ Nice work!</strong> You just added an evaluation layer to your prompt engineering workflow.\n",
    "</div>\n",
    "\n",
    "**Key takeaways**\n",
    "- Layer a rubric-driven judge after every major template to catch silent failures.\n",
    "- Use weighted criteria and explicit thresholds so automation can act on the score.\n",
    "- Archive judge outputs to track drift and prove quality to stakeholders.\n",
    "\n",
    "**Next up**\n",
    "1. Run the judge on your Activity 3.2 and 3.2 outputs.\n",
    "2. Feed low-scoring responses back into your template for iteration.\n",
    "3. Integrate the judge call into your CI/CD or agent workflow.\n",
    "\n",
    "<div style=\"margin-top:12px; padding:24px; background:linear-gradient(120deg,#155e75 0%,#0ea5e9 50%,#38bdf8 100%); border-radius:14px; color:#f8fafc; text-align:center; box-shadow:0 10px 24px rgba(15,118,110,0.25);\">\n",
    "  <strong style=\"display:block; font-size:1.15em; margin-bottom:6px;\">‚òï Time for a quick reset?</strong>\n",
    "  <span style=\"font-size:0.98em; line-height:1.6;\">Stretch, hydrate, and come back ready to automate the hand-off to production.</span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
